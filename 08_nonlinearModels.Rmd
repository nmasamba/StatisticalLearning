---
title: "NonLinear Models"
output: html_document
---

In this R Markdown Document, we explore the use of nonlinear models using some 
tools in R

```{r}
require(ISLR)
attach(Wage)
```

Polynomials
-----------

First we will use polynomials and focus on a single predictor, age.
We'll do a regression of the response wage on age, and fit a fourth-degree 
polynomial. Normally that would require building up a matrix with columns
x, x^2,...,x^4 and then using our regression function. But in R, the poly 
function will take care of all of that for us.

```{r}
fit <- lm(wage~poly(age,4), data=Wage)
summary(fit)
```

The first two polynomials are very significant, the third one is also significant but the fourth one can be discarded. This tells us that we could reasonably just use the first three polynomials to fit the model.

Note that the 'poly' function generates a basis of *orthogonal polynomials*.

Let's make a plot of the fitted function, along with the standard errors of the fit.

```{r fig.width=7, fig.height=6}
agelims <- range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
preds <- predict(fit, newdata = list(age=age.grid), se=TRUE)
se.bands <- cbind(preds$fit+2*preds$se, preds$fit-2*preds$se)
plot(age, wage, col="darkgrey")
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, col="blue", lty = 2)
```

In the above example, we did a lot of work to produce our polynomial plot, even going to the lengths of producing our standard error bands ourselves. There are other more direct ways of doing this in R, for example:

```{r}
fita <- lm(wage~age+I(age^2)+I(age^3)+I(age^4), data=Wage)
summary(fita)
```

Here 'I()' is a wrapper function; we need it because 'age^2' means something to the formula language, while 'I(age^2)' is protected.

Notice that using the fit we just did, the first three polynomial degrees are still significant but the coefficients are different. This might seem a little disconcerting at first but we find the fits are the same below.

```{r}
plot(fitted(fit), fitted(fita))
```

By using orthogonal polynomials in this simple way, it turns out that we can separately test for each coefficient. So if we look at the summary again, we can see that the linear, quadratic and cubic terms are significant, but not the quartic.

The drawback with orthogonal polynomials is that they only work with generalised linear models and only with one predictor. More generally, if we want to test whether, say, a one degree polynomial is better than another degree, we need to use the more general ANOVA function.

```{r}
fita <- lm(wage~education, data=Wage)
fitb <- lm(wage~education+age, data = Wage)
fitc <- lm(wage~education+poly(age, 2), data = Wage)
fitd <- lm(wage~education+poly(age, 3), data = Wage)
anova(fita, fitb, fitc, fitd)
```

All the above examples have been using squared error loss and fitting linear regressions. We can, of course, use the same tecnnology to fit other models, such as logistic regressions.

Polynomial Logistic Regression
-------------------------------

Now we fit a logistic regression model to a binary response variable constructed from Wage dataframe. We code the big earners (>250k) as 1, else 0.

```{r}
fit <- glm(I(wage>250) ~ poly(age,3), data = Wage, family = binomial)
summary(fit)
preds <- predict(fit, list(age=age.grid), se=T)
se.bands <- preds$fit + cbind(fit=0, lower=-2*preds$se, upper=2*preds$se)
se.bands[1:5, ]
```

We have done the computations on the logit scale. We usually want computations to be on the probability scale. To transform, we need to apply the inverse logit mapping
$$p=\frac{e^\eta}{1+e^\eta}.$$
(Here we have used the ability of MarkDown to interpret LaTeX expressions.)
We can do this simultaneously for all three columns of 'se.bands':

```{r}
prob.bands <- exp(se.bands)/(1+exp(se.bands))
matplot(age.grid, prob.bands, col="blue", lwd = c(2,1,1), lty=c(1,2,2), type="l", ylim=c(0, .1))
points(jitter(age), I(wage>250)/10, pch="|", cex=.5)
```

Splines
-------
Splines are more flexible than polynomials, but the idea is rather similar. Here we will explore cubic splines.

```{r}
require(splines)
fit <- lm(wage~bs(age, knots = c(25,40,60)), data=Wage)
plot(age, wage, col="darkgrey")
lines(age.grid, predict(fit, list(age=age.grid)), col="darkgreen", lwd=2)
abline(v=c(25,40,60), lty=2, col="darkgreen")
```

As a reminder, splines (in the above case) are smoothed cubic polynomials. The divisions (i.e. knots) are places of discontinuity in the third polynomial, but they are constrained to be continous and have continous first and second derivatives which makes them really smooth - so smooth that the human eye cannot see the discontinuity. The idea is that they are more local, and hence less 'waggy-tailed' than polynomials. They are a nice way of fitting flexible functions.

One step up from fixed knot regression splines are smoothing splines. The smoothing spline does not require knot selection, but it does have a smoothing parameter, which can conveniently be specified via the effective degrees of freedom or 'df'.

fit <- smooth.spline(age, wage, df=16)
lines(fit, col="red", lwd=2)

Or we can use Leave One Out Cross Validation to select the smoothing parameter for us automatically:


fit <- smooth.spline(age, wage, cv=TRUE)
lines(fit, col="purple", lwd=2)
fit


Generalised Additive Models
----------------------------

So far we have focused on fitting models with mostly single nonlinear terms. It is rare that we only work with one predictor, we usually have several predictors we wish to study. The 'gam' package makes it easier to work with multiple nonlinear terms. In addition, it knows how to plot these functions and their standard errors.

```{r fig.width=10, fig.height=5}
require(gam)
gam1 <- gam(wage~s(age, df=4)+s(year, df=4)+education, data = Wage)
par(mfrow=c(1,3))
plot(gam1, se=T)
```

The 'gam' function also works for logistic regression and other kinds of generalised linear models.

```{r}
gam2 <- gam(I(wage>250)~s(age, df=4)+s(year, df=4)+education, data = Wage, family = binomial)
plot(gam2)
```

Let's see if we can get away with a linear (unsmoothed) term for year. We run gam using a linear term for year and use Chi-Squared ANOVA to test if there is a significant difference between the models, and we find there isn't, so a linear term for year would work just as well as a smoothed one.

```{r}
gam2a <- gam(I(wage>250)~s(age, df=4)+year+education, data = Wage, family = binomial)
anova(gam2a, gam2, test="Chisq")
```

One nice feature of the 'gam' function is that it knows how to plot the functions nicely, even for models fit by 'lm' and 'glm'. Here, we plot calling gam, but using natural splines instead of smoothing splines. Notice how the plot automatically shows the standard errors.

```{r fig.width=10, fig.height=5}
par(mfrow=c(1,3))
lm1 <- lm(wage~ns(age, df=4)+ns(year, df=4)+education, data = Wage)
plot.gam(lm1, se=T)
```

