---
title: "Decision Trees"
output: html_document
---

We will start off by looking at the 'Carseats' data using the 'tree' package in R, as in the lab in the book.
We create a binary response variable High (for high sales), turning Sales from a quantitative variable into a categorical one, and we include it in the same dataframe.
```{r}
require(ISLR)
require(tree)
attach(Carseats)
hist(Sales)
High <- ifelse(Sales<=8, "No", "Yes")
Carseats <- data.frame(Carseats, High)
```

Now we fit a tree to these data, and summarise and plot it. Notice that we have to _exclude_ 'Sales' from the right-hand side of the formula, because the response is derived from it.
```{r}
tree.carseats <- tree(High~.-Sales, data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0)
```

For a detailed summary of the tree, print it:
```{r}
tree.carseats
```

Lets create a training and test set (250, 150) split of the 400 observations, grow the tree on the training set, and evaluate its performance on the test set.
```{r}
set.seed(1011)
train <- sample(1:nrow(Carseats), 250)
tree.carseats <- tree(High~.-Sales, Carseats, subset = train)
plot(tree.carseats); text(tree.carseats, pretty=0)
tree.pred <- predict(tree.carseats, Carseats[-train, ], type="class")
with(Carseats[-train, ], table(tree.pred, High))
er_rate <- (72+33)/150
```

The above tree was grown to full depth, and might be too variable. We now use CV to prune it and grow an optimal tree instead. We use misclassification error as the basis for doing the pruning. The procedure below will do tenfold cross validation for us.
```{r}
cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass)
cv.carseats
plot(cv.carseats)
prune.carseats <- prune.misclass(tree.carseats, best = 13)
plot(prune.carseats); text(prune.carseats, pretty = 0)
```

Now lets evaluate this pruned tree on the test data.
```{r}
tree.pred <- predict(prune.carseats, Carseats[-train,],type="class")
with(Carseats[-train,], table(tree.pred, High))
er_rate <- (72+32)/150
```

Tress, particularly if they are shallow, are handy for explaining things to people as they are very intuitive. However, they typically don't give very good prediction errors. In the next session, we will look at random forests and boosting, which tend to outperform trees as far as prediction and misclassification errors are concerned.

